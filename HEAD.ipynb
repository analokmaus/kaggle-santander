{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547adc83fd8a4f73aa1c994f49d5be8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, gc\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()\n",
    "from multiprocessing import cpu_count, current_process\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/CL4US/.pyenv/versions/finowcast/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "/Users/CL4US/.pyenv/versions/finowcast/lib/python3.6/site-packages/chainer/_environment_check.py:37: UserWarning: Accelerate has been detected as a NumPy backend library.\n",
      "vecLib, which is a part of Accelerate, is known not to work correctly with Chainer.\n",
      "We recommend using other BLAS libraries such as OpenBLAS.\n",
      "For details of the issue, please see\n",
      "https://docs.chainer.org/en/stable/tips.html#mnist-example-does-not-converge-in-cpu-mode-on-mac-os-x.\n",
      "\n",
      "Please be aware that Mac OS X is not an officially supported OS.\n",
      "\n",
      "  ''')  # NOQA\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import feather\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier, Pool, cv\n",
    "from lightgbm import LGBMClassifier\n",
    "from kuma_utils.BearBoost import BearBoostStacking\n",
    "from kuma_utils.ModelTuna import model_tuna, cross_validate\n",
    "from kuma_utils.PreProcessing import *#adversarial_validation, target_encoding, KMeansFeaturizer, parallel_apply, thread_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_data = '/mnt/gcs-bucket/kaggle/santander/'\n",
    "dir_data = 'rawdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_hdf(os.path.join(dir_data, 'train.hdf'))\n",
    "test_data = pd.read_hdf(os.path.join(dir_data, 'test.hdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_lb_idx = np.load(os.path.join(dir_data, 'public_LB.npy'))\n",
    "private_lb_idx = np.load(os.path.join(dir_data, 'private_LB.npy'))\n",
    "synthesized_idx = np.load(os.path.join(dir_data, 'synthetic_samples_indexes.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data processing\n",
    "## DenoisingAutoEncoder\n",
    "**Not** working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = rank_gauss(train_data, skip_cols=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_train = swap_noise(train_data.drop('target', axis=1).values)\n",
    "noised_train = noised_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import Variable, Chain, optimizers\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import training, iterators, reporter\n",
    "from chainer.training import extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class easyAE(Chain):\n",
    "    def __init__(self):\n",
    "        super(easyAE, self).__init__(\n",
    "            l1=L.Linear(len(FEATURE_COL),50),\n",
    "            l2=L.Linear(50, 50),\n",
    "            l3=L.Linear(50,len(FEATURE_COL)),\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        y = self.fwd(x)\n",
    "        loss = F.mean_squared_error(y, x)\n",
    "        reporter.report({'loss': loss}, self)\n",
    "        return loss\n",
    "        \n",
    "    def fwd(self, x):\n",
    "        y = F.relu(self.l1(x))\n",
    "        y = F.dropout(F.relu(self.l2(y)))\n",
    "        y = self.l3(y)\n",
    "        return y\n",
    "    \n",
    "    def get_features(self, x):\n",
    "        y = F.relu(self.l1(x))\n",
    "        y = F.relu(self.l2(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = easyAE()\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iterators.SerialIterator(noised_train, 128)\n",
    "updater = training.StandardUpdater(train_iter, optimizer)\n",
    "trainer = training.Trainer(updater, (100, 'epoch'))\n",
    "trainer.extend(extensions.PrintReport(['iteration', 'main/loss', 'elapsed_time']))\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.ProgressBar())\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = Variable(.values)\n",
    "# new_features = ae.get_features(x).data\n",
    "# new_features = pd.DataFrame(data=new_features, index=train_data.index, \n",
    "#                             columns=['ae_{}'.format(i) for i in range(new_features.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = Variable(test_data.values)\n",
    "# test_new_features = ae.get_features(x).data\n",
    "# test_new_features = pd.DataFrame(data=test_new_features, index=test_data.index, \n",
    "#                             columns=['ae_{}'.format(i) for i in range(new_features.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# new_corr = new_features.corr()\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# sns.heatmap(new_corr, vmin=-1.0, vmax=1.0, center=0, annot=True, fmt='.1f', \n",
    "#             xticklabels=new_corr.columns.values, yticklabels=new_corr.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Manual feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_hdf(os.path.join(dir_data, 'train.hdf'))\n",
    "test_data = pd.read_hdf(os.path.join(dir_data, 'test.hdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COL = test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_data[['target']], pd.read_hdf(os.path.join(dir_data, 'vert_rank.hdf')), \n",
    "                      left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_data, pd.read_hdf(os.path.join(dir_data, 'cmn_f.hdf')), \n",
    "                      left_index=True, right_index=True)\n",
    "# test_data = pd.merge(test_data, pd.read_hdf(os.path.join(dir_data, 'cnt_f_test.hdf')), \n",
    "#                       left_index=True, right_index=True, how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>cnt_var_190_y</th>\n",
       "      <th>cnt_var_191_y</th>\n",
       "      <th>cnt_var_192_y</th>\n",
       "      <th>cnt_var_193_y</th>\n",
       "      <th>cnt_var_194_y</th>\n",
       "      <th>cnt_var_195_y</th>\n",
       "      <th>cnt_var_196_y</th>\n",
       "      <th>cnt_var_197_y</th>\n",
       "      <th>cnt_var_198_y</th>\n",
       "      <th>cnt_var_199_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_1</th>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_2</th>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_3</th>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_4</th>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 601 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "ID_code                                                                      \n",
       "train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "           var_7   var_8  ...  cnt_var_190_y  cnt_var_191_y  cnt_var_192_y  \\\n",
       "ID_code                   ...                                                \n",
       "train_0  18.6266 -4.9200  ...       0.000015        0.00003       0.000035   \n",
       "train_1  16.5338  3.1468  ...       0.000025        0.00002       0.000030   \n",
       "train_2  14.6155 -4.9193  ...       0.000015        0.00002       0.000015   \n",
       "train_3  14.9250 -5.8609  ...       0.000005        0.00001       0.000020   \n",
       "train_4  19.2514  6.2654  ...       0.000015        0.00002       0.000005   \n",
       "\n",
       "         cnt_var_193_y  cnt_var_194_y  cnt_var_195_y  cnt_var_196_y  \\\n",
       "ID_code                                                               \n",
       "train_0       0.000015       0.000020       0.000020       0.000015   \n",
       "train_1       0.000005       0.000005       0.000010       0.000010   \n",
       "train_2       0.000005       0.000010       0.000010       0.000015   \n",
       "train_3       0.000020       0.000015       0.000035       0.000020   \n",
       "train_4       0.000005       0.000005       0.000025       0.000015   \n",
       "\n",
       "         cnt_var_197_y  cnt_var_198_y  cnt_var_199_y  \n",
       "ID_code                                               \n",
       "train_0       0.000065       0.000025       0.000010  \n",
       "train_1       0.000065       0.000010       0.000005  \n",
       "train_2       0.000040       0.000010       0.000010  \n",
       "train_3       0.000020       0.000010       0.000010  \n",
       "train_4       0.000030       0.000010       0.000010  \n",
       "\n",
       "[5 rows x 601 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()#.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation (shuffling)\n",
    "as reported in discussion, shuffleing data inside a column does not affect prediction ;)\n",
    "**slightly working**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_augmentation(x_train, y_train, x_test, y_test, multiplier=(1, 2)):\n",
    "    data_0 = x_train[y_train == 0]\n",
    "    data_1 = x_train[y_train == 1]\n",
    "    new_x_0 = np.empty((0, x_train.shape[1]))\n",
    "    new_x_1 = np.empty((0, x_train.shape[1]))\n",
    "    new_y_0 = np.zeros(data_0.shape[0] * multiplier[0])\n",
    "    new_y_1 = np.ones(data_1.shape[0] * multiplier[1])\n",
    "    for i in range(multiplier[0]):\n",
    "        new_0 = np.empty_like(data_0)\n",
    "        for col in range(x_train.shape[1]):\n",
    "            new_0[:, col] = np.random.permutation(data_0[:, col])\n",
    "        new_x_0 = np.concatenate([new_x_0, new_0])\n",
    "    for i in range(multiplier[1]):\n",
    "        new_1 = np.empty_like(data_1)\n",
    "        for col in range(x_train.shape[1]):\n",
    "            new_1[:, col] = np.random.permutation(data_1[:, col])\n",
    "        new_x_1 = np.concatenate([new_x_1, new_1])\n",
    "    print('[Shuffle Augmentation]\\ntarget_0: {} -> {}\\ntarget_1: {} -> {}'.format(data_0.shape[0], data_0.shape[0]+new_x_0.shape[0],\n",
    "                                                           data_1.shape[0], data_1.shape[0]+new_x_1.shape[0]))\n",
    "    return np.concatenate([x_train, new_x_0, new_x_1]), np.concatenate([y_train, new_y_0, new_y_1]), x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO this at the beginning is a huge leak!\n",
    "# new_train = stratified_shuffle(train_data)\n",
    "# new_train.shape\n",
    "# train_data = pd.concat([train_data, new_train])\n",
    "# train_data.target.value_counts()\n",
    "# train_data.to_hdf(os.path.join(dir_data, 'train_augmented.hdf', key='data'))\n",
    "# train_data = train_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_features(data, cols):\n",
    "    new_features = pd.DataFrame(index=data.index)\n",
    "    for icol in tqdm(cols):\n",
    "        tmp_data = data.iloc[:, icol+1].copy()\n",
    "        break_point = tmp_data.median()\n",
    "        first_part =  tmp_data[tmp_data < break_point].copy()\n",
    "        second_part =  tmp_data[tmp_data >= break_point].copy()\n",
    "        first_part = pd.Series(np.random.permutation(first_part.values), first_part.index)\n",
    "        second_part = pd.Series(np.random.permutation(second_part.values), second_part.index)\n",
    "        new_data = pd.concat([first_part, second_part])\n",
    "        new_data.name = 'sfl_{}'.format(icol)\n",
    "        new_features = pd.merge(new_features, new_data, left_index=True, right_index=True)\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_features(x_train, y_train, x_test, y_test):\n",
    "    new_train = np.zeros_like(x_train)\n",
    "    new_test = np.zeros_like(x_test)\n",
    "    for icol in tqdm(range(x_train.shape[1]), desc='adding unique value features...'):\n",
    "        tmp_train = np.zeros(x_train.shape[0])\n",
    "        tmp_test = np.zeros(x_test.shape[0])\n",
    "        unique = np.unique(x_train[:, icol][y_train == 1])\n",
    "        tmp_train[np.isin(x_train[:, icol], unique)] = 1\n",
    "        tmp_test[np.isin(x_test[:, icol], unique)] = 1\n",
    "        new_train[:, icol] = tmp_train\n",
    "        new_test[:, icol] = tmp_test\n",
    "    print('new_features: {}'.format(x_train.shape[1] + new_train.shape[1]))\n",
    "    return np.concatenate([x_train, new_train], axis=1), y_train, np.concatenate([x_test, new_test], axis=1), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_features(x_train, y_train, x_test, y_test):\n",
    "#     new_train = np.zeros_like(x_train)\n",
    "#     new_test = np.zeros_like(x_test)\n",
    "#     for icol in tqdm(range(x_train.shape[1]), desc='adding count features...'):\n",
    "#         tmp_train = np.zeros(x_train.shape[0])\n",
    "#         tmp_test = np.zeros(x_test.shape[0])\n",
    "#         cnt_dict = pd.Series(x_train[:, icol])\n",
    "#         tmp_train[np.isin(x_train[:, icol], unique)] = 1\n",
    "#         tmp_test[np.isin(x_test[:, icol], unique)] = 1\n",
    "#         new_train[:, icol] = tmp_train\n",
    "#         new_test[:, icol] = tmp_test\n",
    "#     print('new_features: {}'.format(x_train.shape[1] + new_train.shape[1]))\n",
    "#     return np.concatenate([x_train, new_train], axis=1), y_train, np.concatenate([x_test, new_test], axis=1), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_features(train_df, test_df):\n",
    "    new_train = pd.DataFrame()\n",
    "    new_test = pd.DataFrame()\n",
    "    for col in tqdm(train_df.columns):\n",
    "        if col in ['target']:\n",
    "            continue\n",
    "            \n",
    "        col_all = pd.concat([train_df[col], test_df[col]])\n",
    "        col_name = col + '_cnt'\n",
    "        col_count = col_all.value_counts().to_dict()\n",
    "        \n",
    "        new_train[col_name] = train_df[col].map(col_count).astype(np.int32)\n",
    "        new_test[col_name] = test_df[col].map(col_count).astype(np.int32)\n",
    "    \n",
    "    return new_train, new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83e2d987af643959ebf02b24181862f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnt_f_train, cnt_f_test = count_features(train_data, test_data.iloc[np.concatenate([public_lb_idx, private_lb_idx])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_data, cnt_f_train, left_index=True, right_index=True)\n",
    "test_data = pd.merge(test_data, cnt_f_test, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Learning :)\n",
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC(y_true, y_pred, weight=None, get_false=False):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    return auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_hdf(os.path.join(dir_data, 'train.hdf'))\n",
    "test_data = pd.read_hdf(os.path.join(dir_data, 'test.hdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_data.drop(['target'], axis=1), train_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 400)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190_cnt</th>\n",
       "      <th>var_191_cnt</th>\n",
       "      <th>var_192_cnt</th>\n",
       "      <th>var_193_cnt</th>\n",
       "      <th>var_194_cnt</th>\n",
       "      <th>var_195_cnt</th>\n",
       "      <th>var_196_cnt</th>\n",
       "      <th>var_197_cnt</th>\n",
       "      <th>var_198_cnt</th>\n",
       "      <th>var_199_cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_0</th>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_1</th>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_2</th>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_3</th>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_4</th>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           var_0   var_1    var_2   var_3    var_4   var_5   var_6    var_7  \\\n",
       "ID_code                                                                       \n",
       "train_0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187  18.6266   \n",
       "train_1  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208  16.5338   \n",
       "train_2   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427  14.6155   \n",
       "train_3  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428  14.9250   \n",
       "train_4   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405  19.2514   \n",
       "\n",
       "          var_8   var_9  ...  var_190_cnt  var_191_cnt  var_192_cnt  \\\n",
       "ID_code                  ...                                          \n",
       "train_0 -4.9200  5.7470  ...            3            8            9   \n",
       "train_1  3.1468  8.0851  ...            8            5            9   \n",
       "train_2 -4.9193  5.9525  ...            4            5            6   \n",
       "train_3 -5.8609  8.2450  ...            2            3            6   \n",
       "train_4  6.2654  7.6784  ...            3            8            1   \n",
       "\n",
       "         var_193_cnt  var_194_cnt  var_195_cnt  var_196_cnt  var_197_cnt  \\\n",
       "ID_code                                                                    \n",
       "train_0            4            9            5            5           14   \n",
       "train_1            2            4            4            4           21   \n",
       "train_2            2            2            2            3           12   \n",
       "train_3            4            4            8            5            4   \n",
       "train_4            1            1            9            5            9   \n",
       "\n",
       "         var_198_cnt  var_199_cnt  \n",
       "ID_code                            \n",
       "train_0            5            2  \n",
       "train_1            6            2  \n",
       "train_2            4            2  \n",
       "train_3            2            2  \n",
       "train_4            2            2  \n",
       "\n",
       "[5 rows x 400 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAT_FEATURES = ['var_68']\n",
    "# CAT_FEATURES_IDX = [int(X.columns.get_loc(x)) for x in CAT_FEATURES]\n",
    "# CON_FEATURES_IDX = list(set([x for x in range(X.shape[1])]) - set(CAT_FEATURES_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = StandardScaler()\n",
    "# x_train = pd.DataFrame(sc.fit_transform(x_train), columns=X.columns)\n",
    "# x_test = pd.DataFrame(sc.fit_transform(x_test), columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and Params Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "    'k': 5,\n",
    "    'cat_features': None, \n",
    "    'do_target_encoding': False, \n",
    "    'postprocess': None, \n",
    "    'classification': False,\n",
    "    'split_seed': 2019\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = model_tuna(RandomForestClassifier(), X, y, \n",
    "                 eval_metric=AUC, inverse_metric=True,\n",
    "                 cv_params=cv_params, n_trials=100,\n",
    "                 thread_count=-2, seed=2019, verbose=True, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = model_tuna(CatBoostClassifier(), X, y, \n",
    "                 eval_metric=AUC, inverse_metric=True,\n",
    "                 def_params={'eval_metric': 'AUC', 'max_iteration': 10000}, \n",
    "                 fit_params={'early_stopping_rounds': 1000},\n",
    "                 cv_params=cv_params, n_trials=100,\n",
    "                 thread_count=-2, seed=2019, verbose=True, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Gabriel Preda's kernel\n",
    "LGBM_DEF_PARAMS =  {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.4,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 0.05,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': -1,  \n",
    "    'metric':'auc',\n",
    "    'min_data_in_leaf': 80,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 13,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary', \n",
    "    'verbose': -1,\n",
    "    'num_rounds': 100000\n",
    "}\n",
    "\n",
    "LGBM_FIT_PARAMS = {'verbose': 0, 'early_stopping_rounds': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbc = model_tuna(LGBMClassifier(), X, y, \n",
    "                  eval_metric=AUC, inverse_metric=True,\n",
    "                  def_params=LGBM_DEF_PARAMS, fit_params=LGBM_FIT_PARAMS,\n",
    "                  cv_params=cv_params, n_trials=100,\n",
    "                  thread_count=-2, seed=2019, verbose=True, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = {}\n",
    "lgbc_params = dict(test_params, **LGBM_DEF_PARAMS)\n",
    "lgbc = LGBMClassifier(**lgbc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_wrapper(x_train, y_train, x_test, y_test):\n",
    "    return shuffle_augmentation(x_train, y_train, x_test, y_test, multiplier=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8882c55cb5374c69a1aac639e3d4e9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='adding unique value features...', max=200, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_features: 400\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = unique_features(train_data.values[0:150000, 1:], train_data.values[0:150000, 0], train_data.values[150000:, 1:], train_data.values[150000:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: LGBMClassifier\n",
      "...score: 0.898368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-07ed055ea6e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlgbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m y_pred = cross_validate(lgbc, X, y, eval_metric=AUC, \n\u001b[0;32m----> 3\u001b[0;31m                         \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLGBM_FIT_PARAMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#                         postprocess=unique_features,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#                         cat_features=CAT_FEATURES_IDX, do_target_encoding=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/kaggle/santander/kuma_utils/ModelTuna.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(def_model, X, y, w, eval_metric, k, cat_features, do_target_encoding, postprocess, fit_params, classification, self_prediction, prediction, X_test, return_fit_model, split_seed, return_importance, verbose)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m'LGBM'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             model.fit(x_train, y_train, w_train, eval_set=(\n\u001b[0;32m--> 234\u001b[0;31m                       x_test, y_test), eval_sample_weight=w_test, **fit_params)\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'BearBoostStacking'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/finowcast/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    742\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/finowcast/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/finowcast/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/finowcast/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lgbc.set_params(n_jobs=6)\n",
    "y_pred = cross_validate(lgbc, X, y, eval_metric=AUC, \n",
    "                        fit_params=LGBM_FIT_PARAMS, verbose=True, \n",
    "#                         postprocess=unique_features, \n",
    "#                         cat_features=CAT_FEATURES_IDX, do_target_encoding=True, \n",
    "#                         prediction=True, X_test=test_data.values,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=0.4, bagging_freq=5, boost='gbdt',\n",
       "        boost_from_average='false', boosting_type='gbdt',\n",
       "        class_weight=None, colsample_bytree=1.0, feature_fraction=0.05,\n",
       "        importance_type='split', learning_rate=0.01, max_depth=-1,\n",
       "        metric='auc', min_child_samples=20, min_child_weight=0.001,\n",
       "        min_data_in_leaf=80, min_split_gain=0.0,\n",
       "        min_sum_hessian_in_leaf=10.0, n_estimators=100, n_jobs=6,\n",
       "        num_leaves=13, num_rounds=100000, objective='binary',\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0,\n",
       "        tree_learner='serial', verbose=-1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbc.fit(x_train, y_train, eval_set=(x_test, y_test), **LGBM_FIT_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'imp': lgbc.feature_importances_, 'col': X.columns})\n",
    "_ = importance.plot(kind='barh', x='col', y='imp', figsize=(10, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_learning(X, y, test, chunk_size=1, tuning=True):\n",
    "    chunk_idx = np.arange(0, X.shape[1], chunk_size)\n",
    "    self_stack = {}\n",
    "    y_stack = {}\n",
    "    models = {}\n",
    "    for idx in tqdm(chunk_idx):\n",
    "        print(idx)\n",
    "        train_chunk = X.iloc[:, idx:idx+chunk_size]\n",
    "        test_chunk = test.iloc[:, idx:idx+chunk_size]\n",
    "        model = LGBMClassifier(**{\n",
    "            'task': 'train', 'max_depth': 1, 'boosting_type': 'gbdt',\n",
    "            'objective': 'binary', 'num_leaves': 3, 'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5,\n",
    "            'min_data_in_leaf': 10, \n",
    "#             'lambda_l1': 1, 'lambda_l2': 60, \n",
    "            'n_jobs': 8, 'verbose': -99\n",
    "        })\n",
    "        self_pred, y_pred = cross_validate(model, train_chunk, y, eval_metric=AUC, \n",
    "                                           fit_params={'verbose': 0, 'early_stopping_rounds': 50}, verbose=True, \n",
    "                                           self_prediction=True, k=5, \n",
    "#                                            postprocess=shuffle_augmentation, \n",
    "                                           prediction=True, X_test=test_chunk.values)\n",
    "        self_stack[idx] = self_pred\n",
    "        y_stack[idx] = y_pred\n",
    "        models[idx] = model\n",
    "#         clear_output()\n",
    "    return self_stack, y_stack, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stack, test_stack, chunk_models = chunk_learning(X, y, test_data, chunk_size=1)\n",
    "train_stack = pd.DataFrame().from_dict(train_stack, orient='columns')\n",
    "test_stack = pd.DataFrame().from_dict(test_stack, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['max_stack'] = train_stack.max(axis=1).values\n",
    "X['mean_stack'] = train_stack.mean(axis=1).values\n",
    "X['product_stack'] = (train_stack * 10).product(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['max_stack'] = test_stack.max(axis=1).values\n",
    "test_data['mean_stack'] = test_stack.mean(axis=1).values\n",
    "test_data['product_stack'] = (test_stack * 10).product(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9033558326039407\n"
     ]
    }
   ],
   "source": [
    "# simple sum\n",
    "print(AUC(y, (train_stack * 10).prod(axis=1).values))\n",
    "y_pred = (test_stack * 10).prod(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: Ridge\n",
      "...score: 0.901169\n",
      "...score: 0.897168\n",
      "...score: 0.902672\n",
      "...score: 0.901227\n",
      "...score: 0.904078\n",
      "CV score: 0.901263 +- 0.002310\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "y_pred = cross_validate(Ridge(alpha=0), train_stack, y, eval_metric=AUC, \n",
    "                        verbose=True, prediction=True, X_test=test_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge()\n",
    "ridge.fit(train_stack, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'imp': ridge.coef_, 'col': X.columns})\n",
    "_ = importance.plot(kind='barh', x='col', y='imp', figsize=(10, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(dir_data, 'sample_submission.csv'), index_col=0)\n",
    "# submission.target = y_pred\n",
    "submission = pd.merge(submission.drop('target', axis=1), pred_with_id, left_index=True, right_index=True, how='left')\n",
    "submission = submission.fillna(0)\n",
    "submission.to_csv(os.path.join(dir_data, 'submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08615607, 0.19842205, 0.18825818, ..., 0.00572141, 0.07053014,\n",
       "       0.04228766])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_with_id = pd.Series(y_pred, test_data.index)\n",
    "pred_with_id.name = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
